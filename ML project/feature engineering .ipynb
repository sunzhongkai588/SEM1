{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import nan as NA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =  pd.read_csv(\"train.csv\")\n",
    "#替换缺失值\"？\"为NA\n",
    "train_data.replace(' ?', NA, inplace = True)\n",
    "#返回有nan的列\n",
    "train_data.isna().any()\n",
    "#替换NAN的值为Unknown\n",
    "train_data['workclass'].replace(NA, 'Unknown', inplace = True)\n",
    "train_data['occupation'].replace(NA, 'Unknown', inplace = True)\n",
    "train_data['native-country'].replace(NA, 'Unknown', inplace = True)\n",
    "\n",
    "\n",
    "#国家数据重分类,分为发达国家和发展中国家（不要运行2次）\n",
    "#由于美国人最多，所以将国家缺失值填补为US\n",
    "train_data['native-country'].replace(' United-States', 'US', inplace = True)\n",
    "train_data['native-country'].replace('Unknown', 'US', inplace = True)\n",
    "country = train_data['native-country']\n",
    "country_type = country.unique()\n",
    "for native_country in country_type:\n",
    "    if native_country in [' United States',' United Kingdom',' Germany',' France',' Japan',' Italy',' Canada',' Russia']:\n",
    "        train_data['native-country'].replace(native_country, 'Developed_country', inplace = True)\n",
    "    else:\n",
    "        train_data['native-country'].replace(native_country, 'Uneveloped_country', inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     if native_country != ' United-States' and native_country != 'Unknown':\n",
    "#         train_data['native-country'].replace(native_country, 'Non_US', inplace = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='native-country',y='exceeds50K',data=train_data)\n",
    "# sns.countplot(x='native-country',hue='exceeds50K',data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#按学历，将1为没上过学，2-8为没上过大学，9为高中文凭，10为大学未毕业（可视为高中），11-12为专科，因此，应该重分类\n",
    "def education(education_num):\n",
    "    if education_num == 1:\n",
    "        return 0\n",
    "    elif (education_num > 1) & (education_num < 9):\n",
    "        return 1\n",
    "    elif (education_num >= 9) & (education_num < 11):\n",
    "        return 2\n",
    "    elif (education_num >= 11) & (education_num < 13):\n",
    "        return 3\n",
    "    else:\n",
    "        return (education_num-9)\n",
    "train_data['education-num'] = train_data['education-num'].map(education)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#只能运行一次\n",
    "#将每周工作时长分为三段，一段为小于每周40小时，一段为每周40小时，一段为每周大于40小时\n",
    "def workhours(hours_per_week):\n",
    "    if hours_per_week < 40:\n",
    "        return 0\n",
    "    elif hours_per_week == 40:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "train_data['hours-per-week'] = train_data['hours-per-week'].map(workhours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#性别编码\n",
    "encoder1 = OrdinalEncoder(cols = ['sex']).fit(train_data,train_data.iloc[:,-1]) # 转换sex为 female为1 和 male为2\n",
    "train_data = encoder1.transform(train_data)\n",
    "#国家编码\n",
    "encoder2 = OrdinalEncoder(cols = ['native-country']).fit(train_data,train_data.iloc[:,-1]) # 转换sex为 female为1 和 male为2\n",
    "train_data = encoder2.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Sel=train_data.drop(['fnlwgt','education'],axis=1)\n",
    "\n",
    "#查看各特征与标签的相关性\n",
    "corrDf=pd.DataFrame()\n",
    "corrDf=train_Sel.corr()\n",
    "corrDf['exceeds50K'].sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.heatmap(train_Sel[['exceeds50K','capital-loss','capital-gain','age','hours-per-week','education-num','sex','native-country']].corr(),cmap='BrBG',annot=True,\n",
    "           linewidths=.5)\n",
    "plt.xticks(rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Sel=pd.get_dummies(train_Sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#归一化效果更差\n",
    "# from sklearn import preprocessing\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()  # 也可以用标准化类，然后调用方法\n",
    "# train_Sel[['age','education-num','hours-per-week','capital-gain','capital-loss']] = min_max_scaler.fit_transform(train_Sel[['age','education-num','hours-per-week','capital-gain','capital-loss']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=train_Sel['exceeds50K']\n",
    "x=train_Sel.drop('exceeds50K',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入机器学习算法库\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score,StratifiedKFold\n",
    "from sklearn.ensemble import BaggingClassifier,AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#设置kfold，交叉采样法拆分数据集\n",
    "kfold=StratifiedKFold(n_splits=10)\n",
    "\n",
    "#汇总不同模型算法\n",
    "classifiers=[]\n",
    "classifiers.append(SVC())\n",
    "classifiers.append(DecisionTreeClassifier())\n",
    "classifiers.append(RandomForestClassifier())\n",
    "classifiers.append(ExtraTreesClassifier())\n",
    "classifiers.append(GradientBoostingClassifier())\n",
    "classifiers.append(KNeighborsClassifier())\n",
    "classifiers.append(LogisticRegression())\n",
    "classifiers.append(LinearDiscriminantAnalysis())\n",
    "classifiers.append(BaggingClassifier(base_estimator=SVC()))\n",
    "classifiers.append(AdaBoostClassifier())\n",
    "classifiers.append(MLPClassifier())\n",
    "\n",
    "#不同机器学习交叉验证结果汇总\n",
    "cv_results=[]\n",
    "for classifier in classifiers:\n",
    "    cv_results.append(cross_val_score(classifier,x,y,\n",
    "                                      scoring='f1',cv=kfold,n_jobs=-1))\n",
    "\n",
    "#求出模型得分的均值和标准差\n",
    "cv_means=[]\n",
    "cv_std=[]\n",
    "for cv_result in cv_results:\n",
    "    cv_means.append(cv_result.mean())\n",
    "    cv_std.append(cv_result.std())\n",
    "    \n",
    "#汇总数据\n",
    "cvResDf=pd.DataFrame({'cv_mean':cv_means,\n",
    "                     'cv_std':cv_std,\n",
    "                     'algorithm':['SVC','DecisionTreeCla','RandomForestCla','ExtraTreesCla',\n",
    "                                  'GradientBoostingCla','KNN','LR','LinearDiscrimiAna',\n",
    "                                  'BaggingClassifier','AdaBoostClassifier','MLPClassifier']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvResFacet=sns.FacetGrid(cvResDf.sort_values(by='cv_mean',ascending=False),sharex=False,\n",
    "            sharey=False,aspect=2)\n",
    "cvResFacet.map(sns.barplot,'cv_mean','algorithm',**{'xerr':cv_std},\n",
    "               palette='muted')\n",
    "cvResFacet.set(xlim=(0.7,0.9))\n",
    "cvResFacet.add_legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GradientBoostingClassifier模型\n",
    "GBC = GradientBoostingClassifier()\n",
    "gb_param_grid = {'loss' : [\"deviance\"],\n",
    "              'n_estimators' : [100,200,300],\n",
    "              'learning_rate': [0.1, 0.05, 0.01],\n",
    "              'max_depth': [4, 8],\n",
    "              'min_samples_leaf': [100,150],\n",
    "              'max_features': [0.3, 0.1] \n",
    "              }\n",
    "modelgsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, \n",
    "                                     scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n",
    "modelgsGBC.fit(x,y)\n",
    "\n",
    "#LogisticRegression模型\n",
    "modelLR=LogisticRegression()\n",
    "LR_param_grid = {'C' : [1,2,3],\n",
    "                'penalty':['l1','l2']}\n",
    "modelgsLR = GridSearchCV(modelLR,param_grid = LR_param_grid, cv=kfold, \n",
    "                                     scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n",
    "modelgsLR.fit(x,y)\n",
    "\n",
    "#Adaboost模型\n",
    "Adaboost = AdaBoostClassifier()\n",
    "ab_param_grid = {'n_estimators' : [100,150,200],\n",
    "                 'learning_rate': [0.2, 0.1, 0.05]\n",
    "                }\n",
    "modelgsAB = GridSearchCV(Adaboost, param_grid = ab_param_grid, cv = kfold,\n",
    "                                     scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n",
    "modelgsAB.fit(x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelgsGBC模型\n",
    "print('modelgsGBC模型得分为：%.3f'%modelgsGBC.best_score_)\n",
    "#modelgsLR模型\n",
    "print('modelgsLR模型得分为：%.3f'%modelgsLR.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf8\n",
    " \n",
    " \n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    " \n",
    " \n",
    "'''模型融合中使用到的各个单模型'''\n",
    "clfs = [RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n",
    "        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),\n",
    "        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),\n",
    "        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)]\n",
    " \n",
    "'''切分一部分数据作为测试集'''\n",
    "X, X_predict, y, y_predict = train_test_split(data, target, test_size=0.33, random_state=2017)\n",
    " \n",
    " \n",
    "dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "dataset_blend_test = np.zeros((X_predict.shape[0], len(clfs)))\n",
    " \n",
    "'''5折stacking'''\n",
    "n_folds = 5\n",
    "skf = list(StratifiedKFold(y, n_folds))\n",
    "for j, clf in enumerate(clfs):\n",
    "    '''依次训练各个单模型'''\n",
    "    # print(j, clf)\n",
    "    dataset_blend_test_j = np.zeros((X_predict.shape[0], len(skf)))\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        '''使用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征。'''\n",
    "        # print(\"Fold\", i)\n",
    "        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_submission = clf.predict_proba(X_test)[:, 1]\n",
    "        dataset_blend_train[test, j] = y_submission\n",
    "        dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, 1]\n",
    "    '''对于测试集，直接用这k个模型的预测值均值作为新的特征。'''\n",
    "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)\n",
    "    print(\"val auc Score: %f\" % roc_auc_score(y_predict, dataset_blend_test[:, j]))\n",
    "# clf = LogisticRegression()\n",
    "clf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)\n",
    "clf.fit(dataset_blend_train, y)\n",
    "y_submission = clf.predict_proba(dataset_blend_test)[:, 1]\n",
    " \n",
    "print(\"Linear stretch of predictions to [0,1]\")\n",
    "y_submission = (y_submission - y_submission.min()) / (y_submission.max() - y_submission.min())\n",
    "print(\"blend result\")\n",
    "print(\"val auc Score: %f\" % (roc_auc_score(y_predict, y_submission)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testdata\n",
    "\n",
    "\n",
    "test =  pd.read_csv(\"test.csv\")\n",
    "#替换缺失值\"？\"为NA\n",
    "test.replace(' ?', NA, inplace = True)\n",
    "#返回有nan的列\n",
    "test.isna().any()\n",
    "#替换NAN的值为Unknown\n",
    "test['workclass'].replace(NA, 'Unknown', inplace = True)\n",
    "test['occupation'].replace(NA, 'Unknown', inplace = True)\n",
    "test['native-country'].replace(NA, 'Unknown', inplace = True)\n",
    "\n",
    "\n",
    "#国家数据重分类,分为发达国家和发展中国家（不要运行2次）\n",
    "#由于美国人最多，所以将国家缺失值填补为US\n",
    "test['native-country'].replace(' United-States', 'US', inplace = True)\n",
    "test['native-country'].replace('Unknown', 'US', inplace = True)\n",
    "country = test['native-country']\n",
    "country_type = country.unique()\n",
    "for native_country in country_type:\n",
    "    if native_country in [' United States',' United Kingdom',' Germany',' France',' Japan',' Italy',' Canada',' Russia']:\n",
    "        test['native-country'].replace(native_country, 'Developed_country', inplace = True)\n",
    "    else:\n",
    "        test['native-country'].replace(native_country, 'Uneveloped_country', inplace = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     if native_country != ' United-States' and native_country != 'Unknown':\n",
    "#         train_data['native-country'].replace(native_country, 'Non_US', inplace = True)\n",
    "\n",
    "\n",
    "#按学历，将1为没上过学，2-8为没上过大学，9为高中文凭，10为大学未毕业（可视为高中），11-12为专科，因此，应该重分类\n",
    "def education(education_num):\n",
    "    if education_num == 1:\n",
    "        return 0\n",
    "    elif (education_num > 1) & (education_num < 9):\n",
    "        return 1\n",
    "    elif (education_num >= 9) & (education_num < 11):\n",
    "        return 2\n",
    "    elif (education_num >= 11) & (education_num < 13):\n",
    "        return 3\n",
    "    else:\n",
    "        return (education_num-9)\n",
    "test['education-num'] = test['education-num'].map(education)\n",
    "\n",
    "#只能运行一次\n",
    "#将每周工作时长分为三段，一段为小于每周40小时，一段为每周40小时，一段为每周大于40小时\n",
    "def workhours(hours_per_week):\n",
    "    if hours_per_week < 40:\n",
    "        return 0\n",
    "    elif hours_per_week == 40:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "test['hours-per-week'] = test['hours-per-week'].map(workhours)\n",
    "\n",
    "#性别编码\n",
    "encoder1 = OrdinalEncoder(cols = ['sex']).fit(test,test.iloc[:,-1]) # 转换sex为 female为1 和 male为2\n",
    "test = encoder1.transform(test)\n",
    "#国家编码\n",
    "encoder2 = OrdinalEncoder(cols = ['native-country']).fit(test,test.iloc[:,-1]) # 转换sex为 female为1 和 male为2\n",
    "test = encoder2.transform(test)\n",
    "\n",
    "test_Sel=test.drop(['fnlwgt','education'],axis=1)\n",
    "test_Sel=pd.get_dummies(test_Sel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#归一化效果更差\n",
    "# from sklearn import preprocessing\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()  # 也可以用标准化类，然后调用方法\n",
    "# test_Sel[['age','education-num','hours-per-week','capital-gain','capital-loss']] = min_max_scaler.fit_transform(test_Sel[['age','education-num','hours-per-week','capital-gain','capital-loss']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBCpreData_y=modelgsGBC.predict(test_Sel)\n",
    "GBCpreData_y=GBCpreData_y.astype(int)\n",
    "ID=np.array(range(1,24422))\n",
    "GBCpreResultDf=pd.DataFrame()\n",
    "GBCpreResultDf['id']=ID\n",
    "GBCpreResultDf['prediction']=GBCpreData_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBCpreResultDf.to_csv('GBSmodle.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot('relationship',col='marital-status',data=train_data,kind = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['occupation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data.loc[(train_data['occupation'] == 'Unknown') &(train_data['hours-per-week'] == 0)])['exceeds50K'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['workclass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data.loc[(train_data['workclass']== 'Unknown') &(train_data['occupation']== 'Unknown')])['workclass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[train_data['workclass']==' Never-worked'].replace('Unknown',' No-work',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[(train_data['workclass'] == ' Never-worked') &(train_data['occupation']== 'Unknown')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[train_data['workclass'].isin([' Without-pay',' Never-worked'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[(train_data['workclass']== 'Unknown') & (train_data['age'] < 24) & (train_data['education-num']>=13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
